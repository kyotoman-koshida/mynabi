{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "いままで避けていた「所在地」特徴量について抽出する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = df['所在地']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "どうやらすべて\"東京都\"から始まる文字列である模様"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for loc in locations:\n",
    "    if (loc[:3] != '東京都'):\n",
    "        print(loc[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欠損値もない模様"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "東京都内のどの市区町村なのかを抽出していく。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "区のカテゴリ化を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "カテゴリ化前の訓練データ：\n",
      "  市区町村\n",
      "0    北\n",
      "1   中央\n",
      "2   渋谷\n",
      "3   杉並\n",
      "4   葛飾\n",
      "カテゴリ化前の訓練データの大きさ： 31470\n",
      "カテゴリ化前のテストデータ：\n",
      "  市区町村\n",
      "0  世田谷\n",
      "1   目黒\n",
      "2   豊島\n",
      "3   杉並\n",
      "4   杉並\n",
      "カテゴリ化前のテストデータの大きさ 31262\n",
      "  市区町村\n",
      "0    北\n",
      "1   中央\n",
      "2   渋谷\n",
      "3   杉並\n",
      "4   葛飾\n",
      "the size of merge_wards: 62732\n",
      "len(wards)+len(test_wards)= 62732\n",
      "   市区町村\n",
      "0     1\n",
      "1     2\n",
      "2     3\n",
      "3     4\n",
      "4     5\n",
      "   市区町村\n",
      "0    20\n",
      "1     8\n",
      "2    17\n",
      "3     4\n",
      "4     4\n"
     ]
    }
   ],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "#以下の部分では訓練データの”区”のラベル化に備えて、〇〇区の部分を抽出する。\n",
    "i = 0\n",
    "wards = []\n",
    "for loc in locations:\n",
    "    target1 = \"都\"\n",
    "    idx1 = loc.find(target1)\n",
    "    target2 = \"区\"\n",
    "    idx2 = loc.find(target2)\n",
    "    ward = loc[idx1+1:idx2]\n",
    "    wards.append(ward)\n",
    "\n",
    "wards = pd.DataFrame(wards)\n",
    "wards = wards.rename(columns={0:'市区町村'})#列名の振り直し\n",
    "print(\"カテゴリ化前の訓練データ：\")\n",
    "print(wards.head())\n",
    "print(\"カテゴリ化前の訓練データの大きさ：\",len(wards))\n",
    "\n",
    "\n",
    "#以下の部分ではテストデータの”区”のラベル化に備えて、〇〇区の部分を抽出する。\n",
    "df = pd.read_csv('test.csv')\n",
    "test_locations = df['所在地']\n",
    "\n",
    "i = 0\n",
    "test_wards = []\n",
    "for loc in test_locations:\n",
    "    target1 = \"都\"\n",
    "    idx1 = loc.find(target1)\n",
    "    target2 = \"区\"\n",
    "    idx2 = loc.find(target2)\n",
    "    test_ward = loc[idx1+1:idx2]\n",
    "    test_wards.append(test_ward)\n",
    "\n",
    "test_wards = pd.DataFrame(test_wards)\n",
    "test_wards = test_wards.rename(columns={0:'市区町村'})#列名の振り直し\n",
    "print(\"カテゴリ化前のテストデータ：\")\n",
    "print(test_wards.head())\n",
    "print(\"カテゴリ化前のテストデータの大きさ\",len(test_wards))\n",
    "\n",
    "\n",
    "#カテゴリ化の準備\n",
    "list_cols = ['市区町村']\n",
    "wards_encoder = ce.OrdinalEncoder(cols=list_cols, drop_invariant=True)\n",
    "\n",
    "#訓練データとテストデータで同じカテゴリ番号を付与したいので、訓練用のwardsとテスト用のtest_wardsを一旦結合する\n",
    "merge_wards = pd.concat([wards, test_wards],axis=0)\n",
    "print(merge_wards.head())\n",
    "#以下の二列の値が一致していれば、しっかりと結合できている。\n",
    "print(\"the size of merge_wards:\", len(merge_wards))\n",
    "print(\"len(wards)+len(test_wards)=\",len(wards)+len(test_wards))\n",
    "\n",
    "#いよいよカテゴリ化する。\n",
    "merge_wards = wards_encoder.fit_transform(merge_wards['市区町村'])\n",
    "merge_wards.head(20)\n",
    "\n",
    "#カテゴリ化が完了したので、訓練データとテストデータに再分解\n",
    "wards = merge_wards[:len(wards)]\n",
    "wards.to_csv('wards.csv',index=False)\n",
    "test_wards = merge_wards[len(wards):]\n",
    "test_wards.to_csv('wards.csv',index=False)\n",
    "print(wards.head())\n",
    "print(test_wards.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「アクセス」特徴量について抽出する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最寄り駅のカテゴリ化を試みる。そのため、「所在地」と同様にして訓練データとテストデータを結合する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmath import nan\n",
    "\n",
    "\n",
    "#まずは訓練データについて最寄り駅と所要時間について取得する。\n",
    "df = pd.read_csv('train.csv')\n",
    "accesses = df['アクセス']\n",
    "i = 0\n",
    "stations = []\n",
    "minits = []\n",
    "target1 = '\\t'\n",
    "target2 = '駅'\n",
    "target3 = '歩'\n",
    "target4 = '分'\n",
    "for access in accesses:\n",
    "    stations.append([])\n",
    "    minits.append([])\n",
    "    #以下、最寄りの駅の抽出\n",
    "    idx1 = access.find(target1)\n",
    "    idx2 = access.find(target2)\n",
    "    stations[i].append(access[idx1+1:idx2])\n",
    "    \n",
    "    #以下、徒歩〇分の抽出\n",
    "    idx1 = access.find(target3)\n",
    "    idx2 = access.find(target4)\n",
    "    try:\n",
    "        minits[i].append(int (access[idx1+1:idx2]) )\n",
    "    except:\n",
    "        minits[i].append(nan)\n",
    "\n",
    "    i+=1\n",
    "\n",
    "stations = pd.DataFrame(stations)\n",
    "minits = pd.DataFrame(minits)\n",
    "minits = minits.rename(columns={0:'所要時間'})#列名の振り直し\n",
    "minits.to_csv('minits.csv',index=False)\n",
    "\n",
    "\n",
    "#テストデータについて最寄り駅と所要時間を取得する。\n",
    "df = pd.read_csv('test.csv')\n",
    "test_accesses = df['アクセス']\n",
    "i = 0\n",
    "test_stations = []\n",
    "test_minits = []\n",
    "target1 = '\\t'\n",
    "target2 = '駅'\n",
    "target3 = '歩'\n",
    "target4 = '分'\n",
    "for access in test_accesses:\n",
    "    test_stations.append([])\n",
    "    test_minits.append([])\n",
    "    #以下、最寄りの駅の抽出\n",
    "    idx1 = access.find(target1)\n",
    "    idx2 = access.find(target2)\n",
    "    test_stations[i].append(access[idx1+1:idx2])\n",
    "    \n",
    "    #以下、徒歩〇分の抽出\n",
    "    idx1 = access.find(target3)\n",
    "    idx2 = access.find(target4)\n",
    "    try:\n",
    "        test_minits[i].append(int (access[idx1+1:idx2]) )\n",
    "    except:\n",
    "        test_minits[i].append(nan)\n",
    "\n",
    "    i+=1\n",
    "\n",
    "test_stations = pd.DataFrame(test_stations)\n",
    "test_minits = pd.DataFrame(test_minits)\n",
    "test_minits = test_minits.rename(columns={0:'所要時間'})#列名の振り直し\n",
    "test_minits.to_csv('test_minits.csv',index=False)\n",
    "\n",
    "\n",
    "#訓練データとテストデータを結合する。\n",
    "merge_stations = pd.concat([stations, test_stations],axis=0)\n",
    "\n",
    "\n",
    "#カテゴリ化\n",
    "merge_stations = merge_stations.rename(columns={0:'最寄り駅'})#列名の振り直し\n",
    "list_cols = ['最寄り駅']\n",
    "stations_encoder = ce.OrdinalEncoder(cols=list_cols, drop_invariant=True)\n",
    "merge_stations = stations_encoder.fit_transform(merge_stations['最寄り駅'])\n",
    "\n",
    "stations = merge_stations[:len(stations)]\n",
    "stations.to_csv('stations.csv',index=False)\n",
    "test_stations = merge_stations[len(stations):]\n",
    "test_stations.to_csv('test_stations.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以前に抽出しておいた特徴量（「面積」「階数」「間取り」「契約期間」「築年数」）を読み込む。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_size = pd.read_csv('area_size.csv')\n",
    "house_age = pd.read_csv('house_age.csv')\n",
    "n_floor = pd.read_csv('n_floor.csv')\n",
    "room_arrange = pd.read_csv('room_arrange.csv')\n",
    "contract_span = pd.read_csv('contract_span.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目的変数（「賃料」）を読み込む。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "rent = pd.read_csv('rent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "room_arrange_scores = []\n",
    "for ldks in room_arrange['間取り']:\n",
    "    room_arrange_score = 0\n",
    "    for s in ldks:\n",
    "        if s.isdigit():\n",
    "            room_arrange_score += int(s)\n",
    "        elif (s in ['L', 'D', 'K', 'S']):\n",
    "            room_arrange_score += 1\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    room_arrange_scores.append(room_arrange_score)\n",
    "room_arrange_scores = pd.Series(data=room_arrange_scores,name='間取り得点')\n",
    "#room_arrange_scores = room_arrange_scores.rename(columns={0:'間取り得点'})#列名の振り直し\n",
    "room_arrange_scores.to_csv('room_arrange_scores.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "i = 0\n",
    "for s in n_floor[\"所在階\"]:\n",
    "    try:\n",
    "        n_floor[\"所在階\"][i] = re.findall(r\"\\d+\", s)\n",
    "    except:\n",
    "        n_floor[\"所在階\"][i] = nan\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "floor_scores = []\n",
    "Floor_scores = []\n",
    "for n in n_floor[\"所在階\"]:\n",
    "    if (n == \"\"):\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            floor_score = int(n[0])\n",
    "        except:\n",
    "            floor_score = nan\n",
    "        try:\n",
    "            Floor_score = int(n[1])\n",
    "        except:\n",
    "            Floor_score = nan\n",
    "        floor_scores.append(floor_score)\n",
    "        Floor_scores.append(Floor_score)\n",
    "\n",
    "floor_scores = pd.Series(floor_scores)\n",
    "Floor_scores = pd.Series(Floor_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([house_age, area_size, room_arrange_scores, contract_span, floor_scores, Floor_scores, wards, stations, minits], axis=1)\n",
    "y_train = rent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストデータ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_area_size = pd.read_csv('test_area_size.csv')\n",
    "test_house_age = pd.read_csv('test_house_age.csv')\n",
    "test_n_floor = pd.read_csv('test_n_floor.csv')\n",
    "test_room_arrange = pd.read_csv('test_room_arrange.csv')\n",
    "test_contract_span = pd.read_csv('test_contract_span.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "i = 0\n",
    "for s in test_n_floor[\"所在階\"]:\n",
    "    try:\n",
    "        test_n_floor[\"所在階\"][i] = re.findall(r\"\\d+\", s)\n",
    "    except:\n",
    "        test_n_floor[\"所在階\"][i] = nan\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_floor_scores = []\n",
    "test_Floor_scores = []\n",
    "for n in test_n_floor[\"所在階\"]:\n",
    "    if (n == \"\"):\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            test_floor_score = int(n[0])\n",
    "        except:\n",
    "            test_floor_score = nan\n",
    "        try:\n",
    "            test_Floor_score = int(n[1])\n",
    "        except:\n",
    "            test_Floor_score = nan\n",
    "        test_floor_scores.append(test_floor_score)\n",
    "        test_Floor_scores.append(test_Floor_score)\n",
    "\n",
    "test_floor_scores = pd.Series(test_floor_scores,name='所在階')\n",
    "test_floor_scores.to_csv('test_floor_scores.csv',index=False)\n",
    "test_Floor_scores = pd.Series(test_Floor_scores,name='全体の階数')\n",
    "test_Floor_scores.to_csv('test_capital_floor_scores.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_room_arrange_scores = []\n",
    "for ldks in test_room_arrange['間取り']:\n",
    "    test_room_arrange_score = 0\n",
    "    for s in ldks:\n",
    "        if s.isdigit():\n",
    "            test_room_arrange_score += int(s)\n",
    "        elif (s in ['L', 'D', 'K', 'S']):\n",
    "            test_room_arrange_score += 1\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    test_room_arrange_scores.append(test_room_arrange_score)\n",
    "test_room_arrange_scores = pd.Series(data=test_room_arrange_scores,name='間取り得点')\n",
    "#test_room_arrange_scores = test_room_arrange_scores.rename(columns={0:'間取り得点'})#列名の振り直し\n",
    "test_room_arrange_scores.to_csv('test_room_arrange_scores.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.concat([test_house_age, test_area_size, test_room_arrange_scores, test_contract_span, test_floor_scores, test_Floor_scores, test_wards, test_stations, test_minits], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lightGBMに「築年数」「面積」「間取り」「契約期間」「その部屋のある階数」「全体の階数」「所在地」「最寄り駅」「最寄り駅までの所要時間」をいれる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\koshi\\python\\signate\\mynabi\\venv\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\koshi\\python\\signate\\mynabi\\venv\\lib\\site-packages\\lightgbm\\basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['市区町村', '最寄り駅']\n",
      "  _log_warning('categorical_feature in Dataset is overridden.\\n'\n",
      "c:\\Users\\koshi\\python\\signate\\mynabi\\venv\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\koshi\\python\\signate\\mynabi\\venv\\lib\\site-packages\\lightgbm\\basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "c:\\Users\\koshi\\python\\signate\\mynabi\\venv\\lib\\site-packages\\lightgbm\\basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000338 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1089\n",
      "[LightGBM] [Info] Number of data points in the train set: 22029, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 118651.337373\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's rmse: 35666.5\tvalid_1's rmse: 40678.1\n",
      "[20]\ttraining's rmse: 22792.8\tvalid_1's rmse: 30744.4\n",
      "[30]\ttraining's rmse: 18244.7\tvalid_1's rmse: 27457.7\n",
      "[40]\ttraining's rmse: 16313.8\tvalid_1's rmse: 26121.2\n",
      "[50]\ttraining's rmse: 15124.1\tvalid_1's rmse: 25296.8\n",
      "[60]\ttraining's rmse: 14285.3\tvalid_1's rmse: 24719.4\n",
      "[70]\ttraining's rmse: 13634.6\tvalid_1's rmse: 24307.2\n",
      "[80]\ttraining's rmse: 13096.2\tvalid_1's rmse: 24029.2\n",
      "[90]\ttraining's rmse: 12623.5\tvalid_1's rmse: 23792\n",
      "[100]\ttraining's rmse: 12203\tvalid_1's rmse: 23589\n",
      "[110]\ttraining's rmse: 11833.7\tvalid_1's rmse: 23428.9\n",
      "[120]\ttraining's rmse: 11539.1\tvalid_1's rmse: 23270.3\n",
      "[130]\ttraining's rmse: 11264.5\tvalid_1's rmse: 23159.9\n",
      "[140]\ttraining's rmse: 11017.5\tvalid_1's rmse: 23035.9\n",
      "[150]\ttraining's rmse: 10794.7\tvalid_1's rmse: 22941.6\n",
      "[160]\ttraining's rmse: 10540.7\tvalid_1's rmse: 22802.6\n",
      "[170]\ttraining's rmse: 10349.9\tvalid_1's rmse: 22725.5\n",
      "[180]\ttraining's rmse: 10163.7\tvalid_1's rmse: 22660.2\n",
      "[190]\ttraining's rmse: 9989.74\tvalid_1's rmse: 22605.6\n",
      "[200]\ttraining's rmse: 9820.85\tvalid_1's rmse: 22541\n",
      "[210]\ttraining's rmse: 9657.26\tvalid_1's rmse: 22493.8\n",
      "[220]\ttraining's rmse: 9500.56\tvalid_1's rmse: 22437\n",
      "[230]\ttraining's rmse: 9361.77\tvalid_1's rmse: 22370.6\n",
      "[240]\ttraining's rmse: 9220.67\tvalid_1's rmse: 22312\n",
      "[250]\ttraining's rmse: 9105.86\tvalid_1's rmse: 22276.9\n",
      "[260]\ttraining's rmse: 8977.97\tvalid_1's rmse: 22234\n",
      "[270]\ttraining's rmse: 8886.27\tvalid_1's rmse: 22211.8\n",
      "[280]\ttraining's rmse: 8769.93\tvalid_1's rmse: 22149.9\n",
      "[290]\ttraining's rmse: 8654.41\tvalid_1's rmse: 22109.5\n",
      "[300]\ttraining's rmse: 8565.79\tvalid_1's rmse: 22082.4\n",
      "[310]\ttraining's rmse: 8456.48\tvalid_1's rmse: 22037.6\n",
      "[320]\ttraining's rmse: 8370.97\tvalid_1's rmse: 22007.7\n",
      "[330]\ttraining's rmse: 8277.02\tvalid_1's rmse: 21967.9\n",
      "[340]\ttraining's rmse: 8196.13\tvalid_1's rmse: 21929.3\n",
      "[350]\ttraining's rmse: 8119.92\tvalid_1's rmse: 21911.2\n",
      "[360]\ttraining's rmse: 8043.04\tvalid_1's rmse: 21888.6\n",
      "[370]\ttraining's rmse: 7961.41\tvalid_1's rmse: 21871.9\n",
      "[380]\ttraining's rmse: 7876.1\tvalid_1's rmse: 21844.5\n",
      "[390]\ttraining's rmse: 7806.29\tvalid_1's rmse: 21828.8\n",
      "[400]\ttraining's rmse: 7736.18\tvalid_1's rmse: 21825.2\n",
      "[410]\ttraining's rmse: 7673.96\tvalid_1's rmse: 21797.1\n",
      "[420]\ttraining's rmse: 7612.35\tvalid_1's rmse: 21758.6\n",
      "[430]\ttraining's rmse: 7554.7\tvalid_1's rmse: 21746.8\n",
      "[440]\ttraining's rmse: 7493.21\tvalid_1's rmse: 21733.1\n",
      "[450]\ttraining's rmse: 7431.39\tvalid_1's rmse: 21718\n",
      "[460]\ttraining's rmse: 7369.88\tvalid_1's rmse: 21705.3\n",
      "[470]\ttraining's rmse: 7309.02\tvalid_1's rmse: 21686.9\n",
      "[480]\ttraining's rmse: 7241.81\tvalid_1's rmse: 21671.2\n",
      "[490]\ttraining's rmse: 7183.9\tvalid_1's rmse: 21653.6\n",
      "[500]\ttraining's rmse: 7119.87\tvalid_1's rmse: 21628\n",
      "[510]\ttraining's rmse: 7060.58\tvalid_1's rmse: 21614.7\n",
      "[520]\ttraining's rmse: 7000.68\tvalid_1's rmse: 21599.8\n",
      "[530]\ttraining's rmse: 6944.42\tvalid_1's rmse: 21566.2\n",
      "[540]\ttraining's rmse: 6889.06\tvalid_1's rmse: 21543.7\n",
      "[550]\ttraining's rmse: 6839.78\tvalid_1's rmse: 21530.4\n",
      "[560]\ttraining's rmse: 6785.95\tvalid_1's rmse: 21520.7\n",
      "[570]\ttraining's rmse: 6732.26\tvalid_1's rmse: 21492.6\n",
      "[580]\ttraining's rmse: 6686.32\tvalid_1's rmse: 21468.6\n",
      "[590]\ttraining's rmse: 6638.3\tvalid_1's rmse: 21448.3\n",
      "[600]\ttraining's rmse: 6594.64\tvalid_1's rmse: 21415.1\n",
      "[610]\ttraining's rmse: 6553.23\tvalid_1's rmse: 21401.2\n",
      "[620]\ttraining's rmse: 6513.43\tvalid_1's rmse: 21389.5\n",
      "[630]\ttraining's rmse: 6468.8\tvalid_1's rmse: 21377.4\n",
      "[640]\ttraining's rmse: 6423.21\tvalid_1's rmse: 21364.5\n",
      "[650]\ttraining's rmse: 6381.06\tvalid_1's rmse: 21363\n",
      "[660]\ttraining's rmse: 6338.02\tvalid_1's rmse: 21355.1\n",
      "[670]\ttraining's rmse: 6298.46\tvalid_1's rmse: 21347.1\n",
      "[680]\ttraining's rmse: 6254.31\tvalid_1's rmse: 21339.8\n",
      "[690]\ttraining's rmse: 6210.48\tvalid_1's rmse: 21329\n",
      "[700]\ttraining's rmse: 6173.28\tvalid_1's rmse: 21325.6\n",
      "[710]\ttraining's rmse: 6135.76\tvalid_1's rmse: 21311.7\n",
      "[720]\ttraining's rmse: 6104.91\tvalid_1's rmse: 21305.1\n",
      "[730]\ttraining's rmse: 6067.49\tvalid_1's rmse: 21290.6\n",
      "[740]\ttraining's rmse: 6025.84\tvalid_1's rmse: 21286.4\n",
      "[750]\ttraining's rmse: 5991.4\tvalid_1's rmse: 21279.9\n",
      "[760]\ttraining's rmse: 5951.72\tvalid_1's rmse: 21274.7\n",
      "[770]\ttraining's rmse: 5906.52\tvalid_1's rmse: 21269.2\n",
      "[780]\ttraining's rmse: 5867.48\tvalid_1's rmse: 21260.7\n",
      "[790]\ttraining's rmse: 5828.83\tvalid_1's rmse: 21248.8\n",
      "[800]\ttraining's rmse: 5790.4\tvalid_1's rmse: 21238.1\n",
      "[810]\ttraining's rmse: 5751.12\tvalid_1's rmse: 21225.6\n",
      "[820]\ttraining's rmse: 5716.4\tvalid_1's rmse: 21214.1\n",
      "[830]\ttraining's rmse: 5681.61\tvalid_1's rmse: 21210.2\n",
      "[840]\ttraining's rmse: 5644.68\tvalid_1's rmse: 21197.9\n",
      "[850]\ttraining's rmse: 5611.57\tvalid_1's rmse: 21189.2\n",
      "[860]\ttraining's rmse: 5578.24\tvalid_1's rmse: 21181.2\n",
      "[870]\ttraining's rmse: 5548.53\tvalid_1's rmse: 21177.1\n",
      "[880]\ttraining's rmse: 5519.92\tvalid_1's rmse: 21170.6\n",
      "[890]\ttraining's rmse: 5490.01\tvalid_1's rmse: 21158.3\n",
      "[900]\ttraining's rmse: 5457.21\tvalid_1's rmse: 21155.3\n",
      "[910]\ttraining's rmse: 5421.89\tvalid_1's rmse: 21144.6\n",
      "[920]\ttraining's rmse: 5388.61\tvalid_1's rmse: 21136.2\n",
      "Early stopping, best iteration is:\n",
      "[918]\ttraining's rmse: 5393.84\tvalid_1's rmse: 21135.7\n"
     ]
    }
   ],
   "source": [
    "category_lists = ['市区町村','最寄り駅']\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    'objective':'regression',\n",
    "    'metrics':'rmse',\n",
    "    'lambda_l2':0.00001\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "                    params,\n",
    "                    lgb_train, \n",
    "                    valid_sets=[lgb_train, lgb_eval], \n",
    "                    verbose_eval=10, \n",
    "                    num_boost_round=3000, \n",
    "                    early_stopping_rounds=10,\n",
    "                    categorical_feature = category_lists\n",
    "                    )\n",
    "\n",
    "y_pred = model.predict(X_test, num_iteration=model.best_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スコアは下がったが、まだ過学習気味である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.to_csv('result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = df['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([id, y_pred],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('result.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a98cf4a852aef637b580117f8642dec837d1a50d3f811fd67a8b5a8fce3bf795"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
